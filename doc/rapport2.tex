\documentclass[11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{url}
\usepackage{color}
\usepackage{float}
\usepackage{hyperref}
\usepackage{relsize}
% \usepackage{xspace}
\usepackage{bm}
\usepackage{tabularx}

\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}


% \overfullrule=2cm

\newcommand{\husk}[1]{\color{red} #1 \color{black}}
\newcommand{\expect}[1]{\left\langle{#1}\right\rangle}
\newcommand{\bra}[1]{\langle{#1}|}
\newcommand{\ket}[1]{|{#1}\rangle}
\newcommand{\rn}{\mathbf{r}^\text{new}}
\newcommand{\ro}{\mathbf{r}^\text{old}}

\newcommand{\CC}{C\nolinebreak\hspace{-.05em}\raisebox{.4ex}{\tiny\bf +}\nolinebreak\hspace{-.10em}\raisebox{.4ex}{\tiny\bf +}}
\def\CC{{C\nolinebreak[4]\hspace{-.05em}\raisebox{.4ex}{\tiny\bf ++}}}

\title{Project 2 in FYS4411: Computational Physics 2}
\author{Mathias M. Vege}

\date{\today}
\begin{document}
\maketitle

\begin{abstract}
NAN
\end{abstract}

\tableofcontents

\section{Introduction}
The goal of this project is to study closed shell systems of electrons confined in a harmonic oscillator potential - a quantum dot. Within this scope we are investigating the ground state energies, \husk{exception values kinetic and potential energies, single particle densities and one-body densities}. The system we are interested in is a two dimensional system of $N$ electrons, and since we have closed shell systems we will look at $N=2,6$ and $12$ electrons.



\section{Theory}
As tradition demands we begin by looking at the Hamiltonian of the system we are to solve,
\begin{align}
	\hat{H} = \sum^N_{i=1} \left( -\frac{1}{2}\nabla^2_i + \frac{1}{2}\omega^2r^2_i \right) + \sum^N_{i<j} \frac{1}{r_{ij}}
	\label{eq:hamiltonian}
\end{align}
In order to accommodate a modern notational benefits and simplifications, we use natural units($\hbar = c = e = m_e = 1$). We can also observe that $N$ is the number of particles we are using, and the $\omega$ is the oscillator frequency for the harmonic oscillator part. The first part, we recognize as the unperturbed part of the Hamiltonian,
\begin{align}
	\hat{H}_0 = \sum^N_{i=1} \left( -\frac{1}{2}\nabla^2_i + \frac{1}{2}\omega^2r^2_i \right),
	\label{eq:hamiltonian_unperturbed}
\end{align}
and the last part is the perturbation to our system,
\begin{align}
	\hat{H}_1 = \sum^N_{i<j} \frac{1}{r_{ij}}
	\label{eq:hamiltonian_perturbed}
\end{align}
such that $\hat{H} = \hat{H}_0 + \hat{H}_1$. The distance between two electrons is defined as following,
\begin{align}
	r_{ij} = |\mathbf{r}_i - \mathbf{r}_j| = \sqrt{(x_i - x_j)^2 + (y_i - y_j)^2}
	\label{eq:electron_distance}
\end{align}


\subsection{Variational Monte Carlo}
In order to go through the basic steps of variational Monte Carlo, we begin by deriving the basic formulas needed. Let $\psi_T$ be our trial wave function, which we can expand into an energy basis which we assume is normalized,
\begin{align*}
	\psi_T = \sum_{i=0}c_i \ket{\psi_i}
\end{align*}
The expectation value of the energy given by the Hamiltonian $\hat{H}$(from now one omitting the hat) is,
\begin{align*}
	\expect{E} &= \bra{\Psi_T}H\ket{\psi_T}\\ 
	&= \sum_{i=0}\sum_{j=0}c_i^*c_j\bra{\psi_i}H\ket{\psi_j} \\
	&= \sum_{i=0}\sum_{j=0}c_i^*c_j E_j\expect{\psi_i\psi_j} \\
	&= \sum_{i=0}|c_i|^2 E_i
\end{align*}
From the variational principle in quantum mechanics, we have that the energy of the ground state can be approximated by a trial wave function,
\begin{align}
	E_0 \geq E[\psi_T] = \frac{\bra{\psi_T}H\ket{\psi_T}}{\expect{\psi_T|\psi_T}}
	\label{eq:variational-principle}
\end{align}
Writing this on integral form, we get
\begin{align*}
	E[H] = \expect{H} &= \frac{\int d\mathbf{r}\psi_T^*(\mathbf{r})H(\mathbf{r})\psi_T(\mathbf{r}) }{\int d\mathbf{r} \psi_T^*(\mathbf{r})\psi_T(\mathbf{r})} \\
	&=\frac{\sum_{ij} c_i^*c_j\int d\mathbf{r}\psi_i^*(\mathbf{r})H(\mathbf{r})\psi_j(\mathbf{r}) }{\sum_{ij}c_i^*c_j\int d\mathbf{r} \psi_i^*(\mathbf{r})\psi_j(\mathbf{r})} \\
	&= \frac{\sum_i |c_i|^2 E_i}{\sum_i |c_i|^2} \\
	&\geq E_0
\end{align*}
In order to make any progress with variational Monte Carlo, we need to get ourselves a wave function. This wave function will have to take $N$ particles $\mathbf{r}=(\mathbf{r}_1,\mathbf{r}_2,...,\mathbf{r}_N)$, and have can be varied by a set of variational parameters $\alpha=\{\alpha_i\}$. The integral we wish to solve is now,
\begin{align}
	E[H] = \frac{\int d\mathbf{r}\psi_T^*(\mathbf{r},\alpha)H(\mathbf{r})\psi_T(\mathbf{r},\alpha) }{\int d\mathbf{r} \psi_T^*(\mathbf{r},\alpha)\psi_T(\mathbf{r},\alpha)}
	\label{eq:variational-integral}
\end{align}
As with Monte Carlo methods, we need to define ourselves a probability distribution function(PDF) which will be based off our trial wave function $\psi_T(\mathbf{r})$,
\begin{align}
	P(\mathbf{r}) = \frac{|\psi_T(\mathbf{r})|^2}{\int |\psi_T(\mathbf{r})|^2d\mathbf{r}}
	\label{eq:pdf}
\end{align}
We can now define $E_L$ as our local energy,
\begin{align}
	E_L = \frac{1}{\psi_T}H\psi_T
	\label{eq:local-energy}
\end{align}
We can use this and the PDF, we can rewrite our $E[H]$ which now also depends on $\alpha$,
\begin{align*}
	E[H(\alpha)] = \int P(\mathbf{r})E_L(\mathbf{r})d\mathbf{r} \approx \frac{1}{N_{MC}}\sum^{N_{MC}}_{i=1}P(\mathbf{r}_i,\alpha)E_L(\mathbf{r}_i,\alpha)
\end{align*}
where $N_{MC}$ is the number of Monte Carlo cycles which we run for. 
We now have an estimate for the ground state energy from the average of our Monte Carlo samples. The algorithm for the VMC(Variational Monte Carlo) method will be based of the Metropolis algorithm \husk{CITE METROPOLIS!!}, such that a quick review of this is in order.



\subsection{The Metropolis Algorithm}
The Metropolis algorithm \husk{SITER} is a method for obtaining a sequence of samples when direct sampling is problematic. Given a PDF $P^{(n)}_i$, where $n$ is the time step, and $i$ is the current state, the algorithm is such that a transitioning probability to a new state $j$ is given by $T_{i\rightarrow j}$. Then, the probability of accepting this state is given by $A_{i\rightarrow j}$. If rejected, no move is performed and we remain at state $i$. We will require that $A$ and $T$ is such that $P^{(n\rightarrow \infty)}_i\rightarrow p_i$. This gives rise to the relation
\begin{align*}
	P^{(n)}_i &= \sum_j \left[ P^{(n-1)}_j T_{j\rightarrow i} A_{j\rightarrow i} + P^{(n-1)}_i T_{i\rightarrow j} (1 - A_{i\rightarrow j}) \right] \\
	&= \sum_j \left[ P^{(n-1)}_j T_{j\rightarrow i} A_{j\rightarrow i} - P^{(n-1)}_i T_{i\rightarrow j} A_{i\rightarrow j} + P^{(n-1)}_i T_{i\rightarrow j}) \right] \\
	&= \sum_j \left[ P^{(n-1)}_j T_{j\rightarrow i} A_{j\rightarrow i} - P^{(n-1)}_i T_{i\rightarrow j} A_{i\rightarrow j} \right] + P^{(n-1)}_i \sum_j T_{i\rightarrow j}) \\
	&= \sum_j \left[ P^{(n-1)}_j T_{j\rightarrow i} A_{j\rightarrow i} - P^{(n-1)}_i T_{i\rightarrow j} A_{i\rightarrow j} \right] + P^{(n-1)}_i \cdot 1
\end{align*}
Since we require our system to make some transition into a new state $j$, $\sum_j T_{i\rightarrow j} = 1$. Now using the requirement $P^{(n\rightarrow \infty)}_i\rightarrow p_i$, we get 
\begin{align*}
	p_i &= \sum_j \left[ p_j T_{j\rightarrow i} A_{j\rightarrow i} - p_i T_{i\rightarrow j} A_{i\rightarrow j} \right] + p_i \\
	&\downarrow \\
	0 &= \sum_j \left[p_j T_{j\rightarrow i} A_{j\rightarrow i} - p_i T_{i\rightarrow j} A_{i\rightarrow j} \right]
\end{align*}
The requirement applies for the system as a whole when time tends to infinity. This is a rather weak requirement, so we seek to enforce a similar condition to each state. Namely,
\begin{align*}
	p_j T_{j\rightarrow i} A_{j\rightarrow i} = p_i T_{i\rightarrow j} A_{i\rightarrow j}
\end{align*}
which we observe implies that transitioning and accepting a state $i\rightarrow j$ is equal to that of $j \rightarrow i$. By rearranging we get,
\begin{align}
	\frac{A_{j\rightarrow i}}{A_{i\rightarrow j}} = \frac{p_j T_{j\rightarrow i}}{p_i T_{i\rightarrow j}}
	\label{eq:detailed-balance}
\end{align}
This is a much stronger requirement, and is called \textit{detailed balance}. The Metropolis algorithm is now all about maximizing $A$, such that 
\begin{align*}
	A_{i\rightarrow j} = \min \left( 1 , \frac{p_i T_{i\rightarrow j}}{p_j T_{j\rightarrow i}} \right)
\end{align*}
The ratio which we uses to accept or reject a step is defined as
\begin{align}
	q(\mathbf{r}^\text{new},\mathbf{r}^\text{old}) = \frac{p_i T_{i\rightarrow j}}{p_j T_{j\rightarrow i}}
	\label{eq:acceptance-ratio}
\end{align}
Let us then recapitulate what the Metropolis algorithm becomes as applied to our VMC problem.
\begin{algorithm}[H]
	\caption{Metropolis algorithm for Variational Monte Carlo. }
	\label{alg:vmc-metropolis-algorithm}
	\begin{algorithmic}[1]
		\State Set up initial conditions for our system.
		\State Sample a new state $j$ with transition probability $T_{i\rightarrow j}$ by the desired sampling method.
		\State Accept new state $j$ with acceptance probability $A_{i\rightarrow j}$.
		\State If rejected, we go back to state $i$.
		\State Sample the system.
		\State Repeat step 2-5 $N_{MC}$ times.
	\end{algorithmic}
\end{algorithm}

\subsubsection{Uniform sampling}
For sampling a new state(by which we mean a new position), we use
\begin{align}
	\mathbf{r}^\text{new} = \mathbf{r}^\text{old} + \xi \Delta \mathbf{r}^\text{old}
	\label{eq:uniform-sampling}
\end{align}
where $\xi$ is a random number chosen from a uniform distribution, and $\Delta \mathbf{r}$ is the step length which we update for. For this kind of uniform sampling, we seek to tune $\Delta \mathbf{r}$ to such that we accept roughly $50\%$ all suggested moves. 

The acceptance ratio \eqref{eq:acceptance-ratio} as applied to our VMC calculation, is (dubbing it as $R$ instead of $q(\mathbf{r}^\text{new},\mathbf{r}^\text{old})$),
\begin{align}
	R \equiv \frac{\psi^\text{new}_T}{\psi^\text{old}_T}
	\label{eq:vmc-acceptance-ratio-uniform}
\end{align}

\subsubsection{Importance sampling}
If we wish to improve our VMC Metropolis algorithm, we can use something called importance sampling. It is based upon the Fokker-Planck equation, which characterizes a move through coordinate space. For one dimension, it reads
\begin{align}
	\frac{\partial P}{\partial t} = D \frac{\partial}{\partial x} \left( \frac{\partial}{\partial x} - F \right)P(x,t)
	\label{eq:fokker-planck}
\end{align}
The $D$ is a referred to as the diffusion constant since it is derived from the Fokker-Planck equation, and the $F$ is called the drift term. A new position in coordinate space is chosen from solving the Langevin equation,
\begin{align}
	\frac{\partial x(t)}{\partial t} = DF(x(t)) + \eta
	\label{eq:langevin}
\end{align}
$\eta$ is a random variable. This is solved using Euler's method. For our problem, the solution and sampling of a new position is defined by
\begin{align}
	\mathbf{r}^\text{new} = \mathbf{r}^\text{old} + D\mathbf{F}(\mathbf{r}^\text{old})\Delta t + \xi\sqrt{\Delta t}
	\label{eq:importance-sampling}
\end{align}
Where $D=0.5$ in atomic units, and $\mathbf{F}$ is the quantum force as defined by,
\begin{align}
	\mathbf{F} = \frac{2}{\psi_T}\nabla\psi_T
	\label{eq:quantum-force}
\end{align}
We typically choose $\Delta t\in [0.001,0.01]$. The random variable $\xi$ is chosen from a Gaussian distribution around $0$ with a standard deviation of $1$. Further, the acceptance ratio $q(\mathbf{r}^\text{new},\mathbf{r}^\text{old})$ now becomes,
\begin{align}
	q(\mathbf{r}^\text{new},\mathbf{r}^\text{old}) = \frac{G(\mathbf{r}^\text{old},\mathbf{r}^\text{new})|\psi_T(\mathbf{r}^\text{new})|^2}{G(\mathbf{r}^\text{new},\mathbf{r}^\text{old})|\psi_T(\mathbf{r}^\text{old})|^2}
	\label{eq:transition-probability-importance-sampling}
\end{align}
where the $G(\mathbf{r}^\text{new},\mathbf{r}^\text{old})$ is a Green's function which is a solution to the Fokker-Planck equation. This is a transition probability, and is defined by
\begin{align}
	G(\mathbf{r}^\text{new},\mathbf{r}^\text{old}) = \frac{1}{(4\pi D \Delta t)^{3N/2}}\exp \left(-(\rn - \ro - D\delta t \mathbf{F}(\ro))^2/4D\Delta t\right)
	\label{eq:greens-function}
\end{align}
A more detailed derivation of importance sampling can be found in \husk{SITER MORTEN KOMPENDIUM HER}. Is we can already observe, it is possible to simplify the Greens ratio \eqref{eq:transition-probability-importance-sampling} due to the exponential. We will begin by using that the squaring of the exponential implies a dot product for vectors. For simplicity, we dub $\mathbf{x} = \mathbf{r}^\text{old}$ and $\mathbf{y} = \mathbf{r}^\text{new}$. Through a lot of patience, we get \husk{REMOVE THIS???}
\begin{align*}
	\frac{G(\mathbf{x},\mathbf{y})}{G(\mathbf{y},\mathbf{x})} &= \exp \bigg[ \frac{1}{4D\Delta t} \Big\{
	 - \big( x_x^2 - x_x y_x - x_x D\Delta t F_x(\mathbf{y}) + y_x^2 - y_x x_x \\
	&+ y_x D \Delta t F_x (\mathbf{y}) - x_x D \Delta t F_x(\mathbf{y}) + y_x D \Delta t F_x(\mathbf{y}) + D^2 \Delta t^2 F_x^2(\mathbf{y}) \big) \\ 
	&+ \big( y_x^2 - y_x x_x - y_x D \Delta t F_x(\mathbf{x}) + x_x^2 - x_x y_x \\
	&+ x_x D \Delta t F_x(\mathbf{x}) + D^2 \Delta t^2 F_x^2(\mathbf{x}) - y_x D \Delta t F_x(\mathbf{x}) + x_x D \Delta t F_x(\mathbf{x}) \big) \\
	&- \big( x^2_y -x_y y_y - x_y D \Delta t F_y(\mathbf{y}) - x_y y_y + y_y^2 \\
	&+ y_y D \Delta t F_y(\mathbf{y}) - x_y D \Delta t F_y(\mathbf{y}) + y_y D \Delta t F_y(\mathbf{y} + D^2 \Delta t^2 F_y^2 (\mathbf{y})) \big) \\
	&+ \big( y_y^2 - y_y x_y - y_y D \Delta t F_y(\mathbf{x}) + x_y^2 - x_y y_y + x_y D \Delta t F_y(\mathbf{x}) \\
	&+ D^2 \Delta t^2 F_y^2 (\mathbf{x}) - y_y D \Delta t F_y (\mathbf{x}) + x_y D \Delta t F_y (\mathbf{x}) \big) \Big\} \bigg] \\
	&= \exp \bigg[ \frac{1}{4D\Delta t} \Big\{ - y_x D \Delta t F_x(\mathbf{x}) + x_x D \Delta t F_x(\mathbf{x}) + D^2 \Delta t^2 F_x^2(\mathbf{x}) \\
	&- y_x D \Delta t F_x (\mathbf{x}) + x_x D\Delta t F_x(\mathbf{x}) + x_x D \Delta t F_x(\mathbf{y}) - y_x D \Delta t F_x (\mathbf{y}) \\
	&+ x_x D \Delta t F_x (\mathbf{y}) - y_x D \Delta t F_x (\mathbf{y}) - D^2 \Delta t^2 F_x^2(\mathbf{y}) - y_y D \Delta t F_y(\mathbf{x}) \\
	&+ x_y D \Delta t F_y (\mathbf{x}) + D^2 \Delta t^2 F_y^2 (\mathbf{x}) - y_y D \Delta t F_y(\mathbf{x}) + x_y D \Delta t F_y(\mathbf{x}) \\
	&+ x_y D \Delta t F_y (\mathbf{y}) - y_y D \Delta t F_y (\mathbf{y}) + x_y D \Delta t F_y (\mathbf{y}) - y_y D \Delta t F_y(\mathbf{y}) \\
	&- D^2 \Delta t^2 F_y^2(\mathbf{y})\Big\} \bigg] \\
	&= \exp \bigg[ \frac{1}{4D\Delta t} \Big\{ -y_x D\Delta t \left[ F_x(\mathbf{x}) + F_x(\mathbf{x}) + F_x(\mathbf{y}) + F_x(\mathbf{y}) \right] \\
	&- D^2 \Delta t^2 \left[ F_x^2(\mathbf{y}) - F_x^2(\mathbf{x}) \right] + 2x_x D \Delta t \left[ F_x(\mathbf{x}) + F_x (\mathbf{y}) \right] \\
	&- 2y_y D \Delta t \left[ F_y(\mathbf{x}) + F_y(\mathbf{y}) \right] + 2 x_y D \Delta t \left[ F_y(\mathbf{x}) + F_y(\mathbf{y}) \right] \\
	&- D^2 \Delta t^2 \left[ F_y^2(\mathbf{y}) - F_y^2 (\mathbf{x}) \right] \Big\} \bigg] \\
	&= \exp \bigg[ - \frac{y_x}{2}\left( F_x(\mathbf{x}) + F_x(\mathbf{y})\right) - \frac{D \Delta t}{4} \left( F_x^2(\mathbf{y}) - F_x^2(\mathbf{x}) \right) \\
	&+ \frac{x_x}{2}\left(F_x(\mathbf{x}) + F_x (\mathbf{y})\right) - \frac{y_y}{2}\left(F_y(\mathbf{x}) + F_y(\mathbf{y})\right) \\
	&- \frac{D \Delta t}{4} \left(F^2_y(\mathbf{y}) - F_y^2(\mathbf{x})\right) + \frac{x_y}{4}\left( F_y(\mathbf{x}) + F_y (\mathbf{y}) \right)\bigg] \\
\end{align*}
Through the magic of simplification(but mostly patience), we arrive at
\begin{align*}
	\frac{G(\mathbf{x},\mathbf{y})}{G(\mathbf{y},\mathbf{x})} &= \exp\bigg[ \frac{1}{2}(x_x-y_x)\left(F_x(\mathbf{x}) + F_x(\mathbf{y})\right) \\
	&+ \frac{1}{2}\left(x_y - y_y\right)\left( F_y(\mathbf{x}) + F_y(\mathbf{y})\right) \\
	&- \frac{D\Delta t}{4}\left( F^2_x(\mathbf{y}) - F_x^2(\mathbf{x}) + F_y^2(\mathbf{y}) - F_y^2(\mathbf{x}) \right)\bigg]
\end{align*}
Summing over dimensions, and we get
\begin{align}
	\frac{G(\mathbf{x},\mathbf{y})}{G(\mathbf{y},\mathbf{x})} &= \exp\Bigg[ \sum_i^{N_\text{dim}} \frac{1}{2}\left( F_i(\mathbf{x}) + F_i(\mathbf{y}) \right) \left( \frac{D \Delta t}{2}\left(F_i(\mathbf{x}) - F_i(\mathbf{y})\right) - y_i + x_i \right) \Bigg]
	\label{eq:greens-ratio}
\end{align}

\subsubsection{Steepest descent}
The steepest descent method is a simple iterative method for finding a local minimum in parameters. It is used when we are tasked to solve systems of linear equations on the form of
\begin{align*}
	A \mathbf{x} = \mathbf{b}
\end{align*}
where we through an iterative process seeks to minimize the following,
\begin{align*}
	\mathbf{r} = \mathbf{b} - A \mathbf{x}
\end{align*}
where the exact solution implies that $\mathbf{r}=0$. The process for solving this through steepest descent can be set up as, 
\begin{align}
	\mathbf{x}_{i+1} = \mathbf{x}_i - \bm{\nabla} f(\mathbf{x}_i) \delta t
	\label{eq:steepest-descent}
\end{align}
Applied to our VMC problem, where we will have one or two variational parameters, $\alpha$ and $\beta$, the equations will take on the form of \eqref{eq:steepest-descent},
\begin{align}
	\begin{pmatrix}
		\alpha_{i+1} \\
		\beta_{i+1}
	\end{pmatrix}
	=
	\begin{pmatrix}
		\alpha_{i} \\
		\beta_{i}
	\end{pmatrix}
	- \delta t
	\begin{pmatrix}
		\frac{d\langle E_L[\alpha] \rangle}{d\alpha} \\
		\frac{d\langle E_L[\beta] \rangle}{d\beta}
	\end{pmatrix}
\end{align}
Where the $\langle E_L \rangle$ is the expectation value of the local energy and $\delta t$ is the step size of the method. The expectation value of the local energy is found by
\begin{align}
	\frac{d\expect{E_L[\alpha_i]}}{d\alpha_i} &= 2 \left( \expect{ \frac{\bar{\psi}_{\alpha_i}}{\psi_T[\alpha_i]}E_L[\alpha_i] } - \expect{\frac{\bar{\psi}_{\alpha_i}}{\psi[\alpha_i]}}\expect{E_L[\alpha_i]} \right)
	\label{eq:local-energy-variational-derivative}
\end{align}
with the derivative of $\psi$ with respect to the variational parameter is defined as
\begin{align*}
	\bar{\psi}_{\alpha_i} = \frac{\psi[\alpha_i]}{d\alpha_i}
\end{align*}

As evident, this method may oscillate around the solution, since the step size is fixed. Other more optimal methods such as the conjugate gradient method exist\husk{reference?}, but are not implemented in the project.


\subsection{Electron wave function}
Our wave function will consist of two parts: one that comes from the harmonic oscillator potential and is built up based on the fermionic nature of the system, and one that proved a correlation between two particles - the so-called Jastrow factor. The wave function we construct, will be called our \textit{trial wave function}.
\begin{align}
	\psi_T(\mathbf{r}) = \psi_J(\mathbf{r})\psi_{SD}(\mathbf{r})
	\label{eq:WF_trial}
\end{align}

\subsubsection{Slater determinants}
The wave function for an electron in a two dimensional harmonic oscillator potential can be written as a Hermite polynomial,
\begin{align}
	\phi_{n_x,n_y}(x,y) = AH_{n_x} (\sqrt{\omega\alpha}x) H_{n_y} (\sqrt{\omega\alpha} y) \exp{\left(-\frac{\omega\alpha}{2}\left( x^2 + y^2 \right)\right)}
	\label{eq:sp-wf}
\end{align}
The details surrounding the mysterious variational parameter $\alpha$ will be explained further on. For now, we will place these $\phi$'s into a Slater determinant. The Slater determinant is a creature that describes the wave function of a fermionic system, while also enforcing anti-symmetrization and thus the Pauli principle. Our Slater determinant will take the following form, when we describe the specific state of a system by $n_x, n_y$ and a specific particle $\mathbf{r}_i = x_i\mathbf{i} + y_i\mathbf{j}$,
\begin{align}
	D = \det(\Phi(\mathbf{r})) \equiv \frac{1}{\sqrt{N!}}
	\begin{vmatrix}
		\phi_1(\mathbf{r}_1)	& \phi_2(\mathbf{r}_1) 	& \hdots 	& \phi_N(\mathbf{r}_1) 	\\
		\phi_1(\mathbf{r}_2) 	& \phi_2(\mathbf{r}_2) 	& \hdots 	& \phi_N(\mathbf{r}_2) 	\\
		\vdots 					& \vdots				& \ddots 	& \vdots 				\\
		\phi_1(\mathbf{r}_N) 	& \phi_2(\mathbf{r}_N) 	& \hdots 	& \phi_N(\mathbf{r}_N) 	\\
	\end{vmatrix}
	\label{eq:slater_determinant}
\end{align}
From how we have described the position, we introduce following, useful notation, 
\begin{align}
	r_i = \sqrt{x_i^2 + y_i^2}
	\label{eq:position-length}
\end{align}
Note that we have defined $\mathbf{r} \equiv (\mathbf{r_1},\mathbf{r_2},\dots,\mathbf{r_N})$ in the Slater determinant\eqref{eq:slater_determinant}. To pull this definition back to our trial wave function \eqref{eq:WF_trial}, we get
\begin{align}
	\psi_{SD}(\mathbf{r}) = \det(\Phi(\mathbf{r})) = D
	\label{eq:WF_onebody}
\end{align}
Where the $OB$ stands for one body, as in one body wave function. We define an element of the Slater matrix as $d_{ij}$. We now need to look into the part that accounts for many body effects, the Jastrow factor.

\subsubsection{The Jastrow factor}
The correlation term is called a \textit{Jastrow factor} is as mentioned here to take into account many-body effects of our system. The general shape of it is
\begin{align}
	\psi_J(\mathbf{r}) = \prod_{i<j}^N \exp{\left(\frac{ar_{ij}}{1 + \beta r_{ij}}\right)} = \prod_{i=1}^N \prod_{j=i+1}^N \exp{\left(\frac{ar_{ij}}{1 + \beta r_{ij}}\right)}
	\label{eq:WF_jastrow}
\end{align}
where the $C$ stands for correlation. The $a$ is a parameter that is $1$ for parallel spin, and $1/3$ for anti-parallel spin. The $\beta$ is another variational parameter and the $r_{ij}$ is defined by the equation \eqref{eq:electron_distance} as the distance between two electrons. For now, we shall begin by looking closer at the two-electron case.




\subsection{Two electron case}
For the two electron case the Hamiltonian takes on familiar form,
\begin{align}
	H = -\frac{1}{2}\left(\frac{\partial^2}{\partial x^2_1} + \frac{\partial^2}{\partial y^2_1} + \frac{\partial^2}{\partial x^2_2} + \frac{\partial^2}{\partial y^2_2}\right) + \frac{1}{2}\omega^2(x^2_1 + y^2_1 + x^2_2 + y^2_2) + \frac{1}{r_{12}}
	\label{eq:hamiltonian_two_electron}
\end{align}
We will assume the spin is anti-parallel and the total spin is $0$ in the ground state for the $2$ electron case due to Pauli's exclusion principle.

\subsubsection{Unperturbed expectation value}
We begin by looking at the unperturbed expectation value, in which we will try to convince ourselves that the energy comes out to be $2\omega$ in atomic units. Given a wave function,
\begin{align}
	\Phi_0(\mathbf{r}_1,\mathbf{r}_2) = C \exp \left(-\frac{\omega}{2}(r_1^2 + r_2^2)\right)
	\label{eq:two-body-wf}
\end{align}
with $r_1$ and $r_2$ being defined by the notation introduced in the Slater determinant\eqref{eq:position-length}.
\begin{align*}
	\expect{E_0} &= \bra{\Psi_0}H_0\ket{\Psi_0} \\
	&= \int C\exp \left(-\frac{\omega}{2}(r_1^2 + r_2^2)\right) \Big( -\frac{1}{2} \left(\nabla^2_1 + \nabla^2_2 \right) + \frac{1}{2}\omega^2\left( r_1^2 + r_2^2 \right) \Big) \\ &\times C\exp\left(-\frac{\omega}{2}(r_1^2 + r_2^2)\right) d x_1 d y_1 d x_2 d y_2 \\
\end{align*}
Since the derivatives are similar, we can look a single derivative, and then solve for that one.
\begin{align*}
	\frac{\partial^2}{\partial x^2} \exp \left(-\frac{\omega}{2}x^2\right) &= \frac{\partial}{\partial x} \left( -\omega x \right) \exp \left(-\frac{\omega}{2}x^2\right) \\
	&= \left( \omega^2 x^2 - \omega \right) \exp \left(-\frac{\omega}{2}x^2\right)
\end{align*}
Inserting this, into our integral, we get,
\begin{align*}
	I_{x_1} &= -\frac{1}{2}\int \exp \left(-\frac{\omega}{2}\left(x_1^2 + y_1^2 + x_2^2 + y_2^2\right)\right) \frac{\partial^2}{\partial x_1^2} \\
	&\times \exp \left(-\frac{\omega}{2}\left(x_1^2 + y_1^2 + x_2^2 + y_2^2\right)\right) dx_1 dy_1 dx_2 dy_2\\
	&= -\frac{1}{2}\int \left( \omega^2 x_1^2 - \omega \right) \exp \left(-\omega\left(x_1^2 + y_1^2 + x_2^2 + y_2^2\right)\right) dx_1 dy_1 dx_2 dy_2\\
	&= -\frac{\omega}{2}\bigg( \omega \int x^2 \exp \left(-\omega\left(x_1^2 + y_1^2 + x_2^2 + y_2^2\right)\right) dx_1 dy_1 dx_2 dy_2 \\
	&- \int \exp \left(-\omega\left(x_1^2 + y_1^2 + x_2^2 + y_2^2\right)\right) dx_1 dy_1 dx_2 dy_2 \bigg)\\
\end{align*}
We can now use the integral formulas seen in the appendix, integral \eqref{eq:gaussian-integral} and \eqref{eq:gaussian-integral-x-squared}. Noting that there are actually four integrations happening, we get
\begin{align*}
	-\frac{\omega}{2}&\bigg( \omega \int x^2 \exp \left(-\omega\left(x_1^2 + y_1^2 + x_2^2 + y_2^2\right)\right) dx_1 dy_1 dx_2 dy_2 \\
	&- \int \exp \left(-\omega\left(x_1^2 + y_1^2 + x_2^2 + y_2^2\right)\right) dx_1 dy_1 dx_2 dy_2 \bigg) \\
	&= -\frac{\omega}{2}\bigg( \left(\frac{\pi^\frac{1}{2}}{2}\omega^{-\frac{3}{2}}\right)\left(\pi^\frac{1}{2} \omega^{-\frac{1}{2}}\right)^3 - \left(\pi^\frac{1}{2} \omega^{-\frac{1}{2}}\right)^4\bigg) \\
	&= -\frac{\omega}{2}\bigg( \frac{\pi^2}{2}\omega^{-3} + \pi^2\omega^{-2} \bigg) \\
	&= -\frac{\pi^2}{4\omega} + \frac{\pi^2}{2\omega} \\
	&= \frac{\pi^2}{4\omega}
\end{align*}
Putting this together with the other Gaussian symmetric integral coming from the harmonic oscillator, we get
\begin{align*}
	\expect{E_0} &= C^2\left( 4\frac{\pi^2}{4\omega} + 4\frac{\pi^2}{4\omega} \right) \\
	&= C^2 \frac{2\pi^2}{\omega}
\end{align*}
We now need to find the constant C.
\begin{align*}
	\bra{\Phi_0}\ket{\Phi_0} &= C^2\int\exp \left( -\omega\left(x_1^2 + y_1^2 + x_2^2 + y_2^2\right) \right) dx_1dy_1dx_2dy_2 \\
	&= C^2\left( \sqrt{\frac{\pi}{\omega}} \right)^4
\end{align*}
Since our wave function is required to be normalized $C$ becomes,
\begin{align*}
	C = \frac{\omega}{\pi}
\end{align*}
Inserting this, and we get our expectation value
\begin{align*}
	\expect{E_0} &= \frac{\omega^2}{\pi^2}\frac{2\pi^2}{\omega} = 2\omega
\end{align*}
Which, is the answer we expected. This will provide a useful test for us later on.

\subsubsection{Local energy}
We now seek to find an expression for the local energy as given by equation \eqref{eq:local-energy}, for the two electron case.
\begin{align}
	E_L = \frac{1}{\psi_T}H\psi_T
\end{align}
The trial wave function is given as
\begin{align}
	\psi_T = \psi_{SD} \psi_J = C\exp\left( -\frac{\omega \alpha}{2}\left(r_1^2 + r_2^2\right)\right) \exp\left(\frac{ar_{12}}{1+\beta r_{12}} \right)
	\label{eq:trail-WF-2-electron-jastrow}
\end{align}
When the Hamiltonian is given as in equation \eqref{eq:hamiltonian_two_electron}, the main problem is to find the laplacian, 
\begin{align*}
	\nabla^2 = \nabla_1^2 + \nabla_2^2 = \frac{\partial^2}{\partial x_1^2} + \frac{\partial^2}{\partial y_1^2} + \frac{\partial^2}{\partial y_2^2} + \frac{\partial^2}{\partial y_2^2}
\end{align*}
and it is thus where we shall begin. For simplicity, we write
\begin{align*}
	\frac{\nabla^2 \psi_T}{\psi_T} = \frac{\nabla^2 \left( C\exp\left( -\frac{\omega \alpha}{2}\left(r_1^2 + r_2^2\right) + \frac{ar_{12}}{1+\beta r_{12}} \right) \right)}{\psi_T}
\end{align*}
We start by doing a single derivative.
\begin{align}
	\frac{\partial}{\partial x_1} \psi_T = \psi_T\left(-\omega\alpha x_1 + \frac{a(x_1-x_2)}{r_{12}(1+\beta r_{12})^2} \right)
	\label{eq:two-electron-first-derivative}
\end{align}
For the derivative with respect to $x_2$ we get an additional minus sign in the last term. For the second derivative, we get
\begin{align*}
	\frac{\partial^2}{\partial x_1^2} \psi_T &= \frac{\partial}{\partial x_1} \left(\psi_T\left(-\omega\alpha x_1 + \frac{a(x_1-x_2)}{r_{12}(1+\beta r_{12})^2} \right)\right) \\
	&=\psi_T \bigg( \frac{\partial}{\partial x_1}\left(-\omega\alpha x_1\right) + \frac{\partial}{\partial x_1}\left( \frac{a(x_1-x_2)}{r_{12}(1+\beta r_{12})^2} \right) \bigg) \\
	&+ \left(-\omega\alpha x_1 + \frac{a(x_1-x_2)}{r_{12}(1+\beta r_{12})^2}\right) \frac{\partial}{\partial x_1}\left(\psi_T\right) \\
\end{align*}
We now do the second term by itself,
\begin{align*}
	\frac{\partial}{\partial x_1}\left( \frac{a(x_1-x_2)}{r_{12}(1+\beta r_{12})^2} \right) \bigg) &= \frac{a}{r_{12}(1+\beta r_{12})^2} - \frac{a(x_1-x_2)^2}{r_{12}^3(1+\beta r_{12}^2} - \frac{2a\beta(x_1-x_2)^2}{r_{12}^2(1+\beta r_12)^3}
\end{align*}
An overall minus sign appears when taking the derivative with respect to $x_2$. We now look at the last term in our second derivative.
\begin{align*}
	\bigg(-\omega &\alpha x_1 + \frac{a(x_1-x_2)}{r_{12}(1+\beta r_{12})^2} \bigg) \frac{\partial}{\partial x_1}\left(\psi_T\right) = \bigg( \omega^2\alpha^2 x_1^2 - \frac{\omega\alpha a x_1(x_1-x_2)}{r_{12}(1+\beta r_{12})^2} \\
	& - \frac{\omega\alpha a x_1 (x_1 - x_2)}{r_{12}(1+\beta r_{12})^2} + \frac{a^2(x_1 - x_2)^2}{r_{12}^2(1+\beta r_{12})^4}\bigg) \psi_T \\
	&= \bigg( \omega^2\alpha^2 x_1^2 - \frac{2\omega\alpha a x_1(x_1-x_2)}{r_{12}(1+\beta r_{12})^2} + \frac{a^2(x_1 - x_2)^2}{r_{12}^2(1+\beta r_{12})^4}\bigg) \psi_T
\end{align*}
Putting all this together, the second derivative becomes
\begin{align*}
	\frac{\partial^2}{\partial x_1^2} \psi_T &= \bigg( - \omega \alpha + \omega^2\alpha^2 x_1^2 - \frac{2\omega\alpha a x_1(x_1-x_2)}{r_{12}(1+\beta r_{12})^2} + \frac{a^2(x_1 - x_2)^2}{r_{12}^2(1+\beta r_{12})^4} \\
	& + \frac{a}{r_{12}(1+\beta r_{12})^2} - \frac{a(x_1-x_2)^2}{r_{12}^3(1+\beta r_{12}^2} - \frac{2a\beta(x_1-x_2)^2}{r_{12}^2(1+\beta r_12)^3} \bigg) \psi_T \\
	&= \bigg( - \omega \alpha + \omega^2\alpha^2 x_1^2 - \frac{2\omega\alpha a x_1(x_1-x_2)}{r_{12}(1+\beta r_{12})^2} + \frac{2a}{(1+\beta r_{12})^2} 
	\\ &\times \bigg( \frac{1}{2r_{12}} - \frac{(x_1 - x_2)^2}{2 r_{12}^3} - \frac{\beta (x_1 x_2)^2}{r_{12}^2(1+\beta r_{12})} + \frac{a(x_1-x_2)^2}{2r_{12}^2(1+\beta r_{12})^2} \bigg) \bigg) \psi_T
\end{align*}
We can now add the derivative with respect to $x_1$ and $x_2$.
\begin{align*}
	&\left( \frac{\partial^2}{\partial x_1^2} + \frac{\partial^2}{\partial x_1^2} \right)\psi_T = \bigg( - 2\omega \alpha + \omega^2\alpha^2 \left(x_1^2 + x_2^2\right) - \frac{2\omega\alpha a (x_1 - x_2)^2}{r_{12}(1+\beta r_{12})^2} \\
	&+ \frac{2a}{(1+\beta r_{12})^2} \bigg( \frac{1}{r_{12}} - \frac{(x_1 - x_2)^2}{r_{12}^3} - \frac{2\beta (x_1 - x_2)^2}{r_{12}^2(1+\beta r_{12})} + \frac{a(x_1-x_2)^2}{r_{12}^2(1+\beta r_{12})^2} \bigg) \bigg) \psi_T
\end{align*}
Putting this together with the derivatives with respect to $y_1$ and $y_2$ and using the relation given in the beginning \eqref{eq:electron_distance}, we get
\begin{align*}
	\frac{\nabla^2 \psi_T}{\psi_T} &= - 4\omega \alpha + \omega^2 \alpha ^2(r_1^2 + r_2^2) - \frac{2\omega\alpha a r_{12}}{(1+\beta r_{12})^2} \\
	&+ \frac{2a}{(1+\beta r_{12})^2}\bigg( \frac{2}{r_{12}} - \frac{r_{12}^2}{r_{12}^3} - \frac{2\beta r_{12}^2}{r_{12}^2(1+ \beta r_{12})} + \frac{a r_{12}^2}{r_{12}^2(1+\beta r_{12})^2}\bigg)
\end{align*}
which becomes our final expression for the Laplacian,
\begin{align}
	\frac{\nabla^2 \psi_T}{\psi_T} &= \omega^2 \alpha ^2(r_1^2 + r_2^2) - 4\omega \alpha - \frac{2\omega\alpha a r_{12}}{(1+\beta r_{12})^2} \\
	&+ \frac{2a}{(1+\beta r_{12})^2}\bigg(\frac{a }{(1+\beta r_{12})^2} + \frac{1}{r_{12}} - \frac{2\beta}{(1+ \beta r_{12})} \bigg)
	\label{eq:two-electron-laplacian}
\end{align}
Which gives the total local energy by adding the harmonic oscillator term and coulomb interaction,
\begin{align}
	E_L &= -\frac{1}{2}\bigg((\alpha ^2-1)\omega^2(r_1^2 + r_2^2) - 4\omega \alpha - \frac{2\omega\alpha a r_{12}}{(1+\beta r_{12})^2} \\
	&+ \frac{2a}{(1+\beta r_{12})^2}\bigg(\frac{a }{(1+\beta r_{12})^2} + \frac{1}{r_{12}} - \frac{2\beta}{(1+ \beta r_{12})} \bigg)\bigg) + \frac{1}{r_{12}}
	\label{eq:two-electron-local-energy}
\end{align}

\subsubsection{Quantum force}
We also have to find the quantum force given by equation \eqref{eq:quantum-force}, which involves taking the gradient of the wave function. As we already know the first derivatives from taking the gradient\eqref{eq:two-electron-first-derivative}, the process is trivial.
\begin{align}
	\mathbf{F} = \left(-\omega\alpha x_1 + \frac{a(x_1-x_2)}{r_{12}(1+\beta r_{12})^2} \right)\mathbf{i} + \left(-\omega\alpha y_1 + \frac{a(y_1-y_2)}{r_{12}(1+\beta r_{12})^2} \right)\mathbf{j}
	\label{eq:two-electron-quantum-force}
\end{align}
For $r_2$ do we get a minus in front of the last term.

\subsubsection{\texorpdfstring{$\alpha$}{a} and \texorpdfstring{$\beta$}{b} derivatives in the steepest descent algorithm}
For the steepest descent method \eqref{eq:steepest-descent}, we have to find derivatives of the trial wave function \eqref{eq:trail-WF-2-electron-jastrow} with respect to $\alpha$ and $\beta$. 

From the equation for the energy expectation value \eqref{eq:local-energy-variational-derivative}, we see that we have to find the derivative of the wave function\eqref{eq:two-body-wf} with respect to the variational parameters $\alpha$ and $\beta$.
\begin{align}
	\frac{d\psi_T[\alpha]}{d\alpha} = -\frac{\omega}{2}(r_1^2 + r_2^2)\psi_T
	\label{eq:2body-alpha-derivative}
\end{align}
For $\beta$ we get,
\begin{align}
	\frac{d\psi_T[\beta]}{d\beta} = -\frac{a r_{12}^2}{(1+\beta r_{12})^2}\psi_T
	\label{eq:2body-beta-derivative}
\end{align}
Which is what is need in order to apply steepest descent to the two-body Jastrow factor case.

\subsection{\texorpdfstring{$N$}{N} electron case}
For the $N$-electron case, we apply the Hamiltonian as states in equation \eqref{eq:hamiltonian} at the beginning of the theory section on the trial wave function as stated in \eqref{eq:WF_trial}. The $N$ electron energy is once more given as the local energy \eqref{eq:local-energy}. From looking at the general Hamiltonian \eqref{eq:hamiltonian}, we see that a major task will be to once more find the laplacian of our trial wave function \eqref{eq:WF_trial}.
\begin{align}
	\nabla_i^2\psi_T = \frac{\nabla^2_i\psi_{SD}}{\psi_{SD}} + \frac{\nabla^2_i\psi_J}{\psi_J} + 2\frac{\bm{\nabla}_i\psi_{SD}}{\psi_{SD}}\cdot\frac{\bm{\nabla}_i\psi_{J}}{\psi_J}
	\label{eq:laplacian-n-electron}
\end{align}
Since we are calculating the ground state energy, all the lowest lying levels will be filled up according to the Pauli exclusion principle, and the total spin should be zero.

\subsubsection{Jastrow factor derivatives}
We begin by finding the Jastrow terms. The full derivation can be found in the lecture notes for Computational Physics 2 \husk{CITE KOMPENDIEM}. $k$ is here used instead of $i$ as in the equation to the Hamiltonian \eqref{eq:hamiltonian}. Since the Pade-Jastrow factor is an exponential as given in \eqref{eq:WF_jastrow}, we make following definitions,
\begin{align}
	g_{ij} \equiv g(r_{ij}) = e^{f(r_{ij})} = e^{f_{ij}}
	\label{eq:pade-jastrow-eq}
\end{align}
This yields a general equation for the gradient of the Jastrow factor,
\begin{align}
	\frac{1}{\psi_J}\bm{\nabla}_k\psi_J = \sum^{N}_{i\neq k} \frac{1}{g_{ik}} \frac{\mathbf{r}_{ik}}{r_{ik}}\frac{\partial g_{ik}}{\partial r_{ik}} = \sum^N_{i\neq k} \frac{\mathbf{r}_{ik}}{r_{ik}}\frac{\partial f_{ik}}{\partial r_{ik}}
	\label{eq:general-gradient-jastrow}
\end{align}
the $\mathbf{r}_{ik}$ is defined analogues to the electron distance \eqref{eq:electron_distance}, but as
\begin{align}
	\mathbf{r}_{ik} = \mathbf{r}_k - \mathbf{r}_i
	\label{eq:electron-distance-vector}
\end{align}
By taking the first derivative of exponential content in \eqref{eq:WF_jastrow}, we find the derivative of $f_{ij}$ as
\begin{align}
	\frac{\partial f_{ik}}{\partial r_{ik}} = \frac{a_{ik}}{(1+\beta r_{ik})^2}
	\label{eq:jastrow-f-derivative}
\end{align}
By using our previous equation \eqref{eq:general-gradient-jastrow}, we get
\begin{align}
	\frac{1}{\psi_J}\bm{\nabla}_k\psi_J = \sum_{i\neq k}^N \frac{\mathbf{r}_{ik}}{r_{ik}}\frac{a_{ik}}{(1+\beta r_{ik})^2}
	\label{eq:n-body-jastrow-grad}
\end{align}
We can now look for the Laplacian. The general formula for this is one is given by taking the gradient once more,
\begin{align}
	\frac{\bm{\nabla}_k^2\psi_J}{\psi_J} = \left( \frac{\bm{\nabla}_k\psi_J}{\psi_J} \right)^2 + \sum^N_{i\neq k} \left[\left( \frac{d-1}{r_{ik}} \frac{\partial f_{ik}}{\partial r_{ik}} + \frac{\partial^2f_{ik}}{\partial r_{ik}^2} \right)  \right]
	\label{eq:general-laplacian-jastrow}
\end{align}
The $d$ stands for the dimension. The second derivative of $f_{ik}$ is given by
\begin{align}
	\frac{\partial^2f_{ik}}{\partial r_{ik}^2} = -\frac{2a_{ik}\beta}{(1 + \beta r_{ik})^3}
	\label{eq:jastrow-f-second-derivative}
\end{align}
By setting $d=2$(as we only are working in two dimensions), and inserting the partial derivative into the general formula for the Laplacian, we get,
\begin{align}
	\frac{1}{\psi_J}\nabla_k^2 \psi_k = \left( \frac{\bm{\nabla}_k \psi_J}{\psi_J} \right)^2 + \sum_{i\neq k}^N \frac{a_{ik}(1-\beta r_{ik})}{r_{ik}(1+\beta r_{ik})^3}
	\label{eq:n-body-jastrow-lap}
\end{align}

\subsubsection{Slater determinant derivatives}
We can now look at the Slater determinant. As shown in http://onlinelibrary.wiley.com/doi/10.1002/qua.560200508/abstract, we can split the spin parts into two Slater determinants, one for spin up and one for spin down since our Hamiltonian is spin independent. If we do not do this the determinant will be zero. By using the definition we presented in \eqref{eq:WF_onebody}, we get
\begin{align}
	\psi_{SD} = D = D_{\uparrow}D_{\downarrow}
	\label{eq:slater-det-spin-up-down}
\end{align}
This gives following expressions for the gradient,
\begin{align}
	\frac{\bm{\nabla}\psi_{SD}}{\psi_{SD}} = \frac{\bm{\nabla}D_{\downarrow}}{D_{\downarrow}} + \frac{\bm{\nabla}D_{\uparrow}}{D_{\uparrow}}
	\label{eq:slater-gradient-spin-down-up-grad}
\end{align}
and the laplacian
\begin{align}
	\frac{{\nabla}^2\psi_{SD}}{\psi_{SD}} = \frac{{\nabla}^2D_{\downarrow}}{D_{\downarrow}} + \frac{{\nabla}^2D_{\uparrow}}{D_{\uparrow}}
	\label{eq:slater-laplacian-spin-down-up-lap}
\end{align}
The gradient of the Slater determinant is given in the \husk{lecture notes 2015} as
\begin{align}
	\frac{\bm{\nabla}_k\psi_{SD}}{\psi_{SD}} = \sum_{i\neq k}^N\bm{\nabla}_k d_{ik}(\mathbf{r})d_{ki}^{-1}(\mathbf{r})
	\label{eq:gradient-slater}
\end{align}
Since $d_{ik}=\phi_k (\mathbf{r}_i)$, we must find the derivative of the single particle wave function\eqref{eq:sp-wf}. The gradient of this is given as,
\begin{align*}
	\bm{\nabla}_i \phi_j(\mathbf{r}_i) &= \left( \frac{\partial}{\partial x_i} \mathbf{i} + \frac{\partial}{\partial y_i} \mathbf{j} \right) AH_{n_x} (\sqrt{\omega\alpha}x) H_{n_y} (\sqrt{\omega\alpha} y) \exp{\left(-\frac{\omega\alpha}{2}\left( x^2 + y^2 \right)\right)} \\
\end{align*}
Only looking at the $x$-derivative and ignoring the constant $A$ gives us,
\begin{align*}
	\frac{\partial}{\partial x_i} \phi_j(\mathbf{r}_i) &= \frac{\partial}{\partial x_i} \left(H_{j_x} (\sqrt{\omega\alpha}x_i) H_{j_y} (\sqrt{\omega\alpha} y_i) \exp{\left(-\frac{\omega\alpha}{2}\left( x_i^2 + y_i^2 \right)\right)} \right) \\
	&= \bigg[ H_{j_y} (\sqrt{\omega\alpha} y_i) \exp{\left(-\frac{\omega\alpha}{2}\left( x^2 + y^2 \right)\right)}\frac{\partial}{\partial x_i}\left( H_{j_x} (\sqrt{\omega\alpha}x_i) \right) \\
	&+ H_{j_x} (\sqrt{\omega\alpha}x_i) H_{j_y} (\sqrt{\omega\alpha} y_i) (-\omega\alpha x_i) \exp{\left(-\frac{\omega\alpha}{2}\left( x_i^2 + y_i^2 \right)\right)} \bigg] \\
	&= \left[ \frac{\partial}{\partial x_i}\left( H_{j_x} (\sqrt{\omega\alpha}x_i) \right) - \omega\alpha x_i H_{j_x} (\sqrt{\omega\alpha}x_k) \right] \\
	&\times H_{j_y} (\sqrt{\omega\alpha} y_i)\exp{\left(-\frac{\omega\alpha}{2}\left( x_i^2 + y_i^2 \right)\right)}
\end{align*}
Adding this together with the $y$-derivative, and we get the gradient of the single particle wave function,
\begin{align}
	\begin{split}
		\bm{\nabla}_i \phi_j (\mathbf{r}_i) &= \bigg[ \left( \frac{\partial}{\partial x_i}\left( H_{j_x} (\sqrt{\omega\alpha}x_i) \right) - \omega\alpha x_i H_{j_x} (\sqrt{\omega\alpha}x_i) \right) H_{j_y} (\sqrt{\omega\alpha} y_i) \mathbf{i} \\
		&+ \left( \frac{\partial}{\partial y_i}\left( H_{j_y} (\sqrt{\omega\alpha}y_i) \right) - \omega\alpha y_i H_{j_y} (\sqrt{\omega\alpha}y_i) \right) H_{j_x} (\sqrt{\omega\alpha} x_i) \mathbf{j}	\bigg] \\
		& \times \exp{\left(-\frac{\omega\alpha}{2}\left( x_i^2 + y_i^2 \right)\right)}
	\end{split}
	\label{eq:wf-sp-gradient}
\end{align}
We can now find the Laplacian of the Slater determinant. 
\begin{align}
	\frac{\nabla^2_k\psi_{SD}}{\psi_{SD}} = \sum_{i\neq k}^N\bm{\nabla}^2_k d_{ik}(\mathbf{r})d_{ki}^{-1}(\mathbf{r})
	\label{eq:laplacian-slater}
\end{align}
We now need to find the Laplacian of the single particle function.
\begin{align*}
	\frac{\partial^2}{\partial x_i^2} \phi_j(\mathbf{r}_i) &= \frac{\partial}{\partial x_i} \bigg(\left[ \frac{\partial}{\partial x_i}\left( H_{j_x} (\sqrt{\omega\alpha}x_i) \right) - \omega\alpha x_i H_{j_x} (\sqrt{\omega\alpha}x_k) \right] \\
	&\times H_{j_y} (\sqrt{\omega\alpha} y_i)\exp{\left(-\frac{\omega\alpha}{2}\left( x_i^2 + y_i^2 \right)\right)} \bigg) \\
	&= \bigg( \frac{\partial^2}{\partial x_i^2} \left( H_{j_x} (\sqrt{\omega\alpha}x_i) \right) - 2\omega\alpha x_i \frac{\partial}{\partial x_i}\left( H_{j_x} (\sqrt{\omega\alpha}x_i) \right) \\ 
	&- \omega \alpha H_{j_x} (\sqrt{\omega\alpha}x_i) + \omega^2 \alpha^2 x_i^2 H_{j_x} (\sqrt{\omega\alpha}x_i) \bigg) \\
	&\times H_{j_y} (\sqrt{\omega\alpha} y_i) \exp{\left(-\frac{\omega\alpha}{2}\left( x_i^2 + y_i^2 \right)\right)}
\end{align*}
Combinging the second derivatives of respectively $x_i$ and $y_i$, we get
\begin{align}
	\begin{split}
		\nabla_i^2 \phi_j(\mathbf{r}_i) &= \bigg(
			H_{j_y} (\sqrt{\omega\alpha} y_i) \frac{\partial^2}{\partial x_i^2} \left( H_{j_x} (\sqrt{\omega\alpha}x_i) \right) \\
			&+ H_{j_x} (\sqrt{\omega\alpha}x_i) \frac{\partial^2}{\partial y_i^2} \left( H_{j_y} (\sqrt{\omega\alpha}y_i) \right) \\
			&- 2\omega\alpha \bigg[ x_i H_{j_y} (\sqrt{\omega\alpha} y_i) \frac{\partial}{\partial x_i}\left( H_{j_x} (\sqrt{\omega\alpha}x_i) \right) \\
			&+ y_i H_{j_x} (\sqrt{\omega\alpha}x_i) \frac{\partial}{\partial y_i}\left( H_{j_y} (\sqrt{\omega\alpha}y_i) \right) \bigg] \\
			&- 2\omega\alpha H_{j_x} (\sqrt{\omega\alpha}x_i) H_{j_y} (\sqrt{\omega\alpha} y_i) \\
			&+ \omega^2 \alpha^2(x_i^2+y_i^2) H_{j_x} (\sqrt{\omega\alpha}x_i) H_{j_y} (\sqrt{\omega\alpha} y_i)
		\bigg) \exp{\left(-\frac{\omega\alpha}{2}\left( x_i^2 + y_i^2 \right)\right)}
	\end{split}
	\label{eq:wf-sp-laplacian}
\end{align}
We now have all the ingredients to find the local energy as given by equation \eqref{eq:local-energy}.

% H_{j_x} (\sqrt{\omega\alpha}x_i)
% \frac{\partial}{\partial x_i}\left( H_{j_x} (\sqrt{\omega\alpha}x_i) \right)
% \frac{\partial^2}{\partial x_i^2} \left( H_{j_x} (\sqrt{\omega\alpha}x_i) \right)

% H_{j_y} (\sqrt{\omega\alpha} y_i)
% \frac{\partial}{\partial y_i}\left( H_{j_y} (\sqrt{\omega\alpha}y_i) \right)
% \frac{\partial^2}{\partial y_i^2} \left( H_{j_y} (\sqrt{\omega\alpha}y_i) \right)

% \exp{\left(-\frac{\omega\alpha}{2}\left( x_i^2 + y_i^2 \right)\right)}

% \frac{\partial}{\partial x_i}\left(  \right)




\subsubsection{Local energy}
The local energy is given by
\begin{align}
	E_L = \frac{1}{\psi_T}\left( -\frac{1}{2}\sum^N_{i=1}\nabla_i^2 + \frac{1}{2}\omega^2\sum^N_{i=1}r^2_i + \sum^N_{i\leq j}r_{ij} \right)\psi_T
	\label{eq:n-electron-local-energy}
\end{align}
Where the Laplacian is defined as \eqref{eq:laplacian-n-electron}. By using the gradients and Laplacians we found for the Jastrow(\eqref{eq:general-gradient-jastrow},\eqref{eq:general-laplacian-jastrow}), Slater determinant(\eqref{eq:gradient-slater},\eqref{eq:laplacian-slater}) and the single particle wave functions(\eqref{eq:wf-sp-gradient},\eqref{eq:wf-sp-laplacian}), we will have the full local energy for $N$ electrons. This equation is a beast in the true meaning of the word, and we will therefore not write it in its full length. 

\subsubsection{Quantum force}
Finding the quantum force as stated in equation \eqref{eq:quantum-force} is now a simple task, as we already know all the gradients needed, $\bm{\nabla_i}\psi_T = \bm{\nabla}_i\psi_{SD} + \bm{\nabla}_i\psi_J$.
\begin{align}
	\mathbf{F_k} = 2 \left( \sum^N_{i\neq k} \frac{\mathbf{r}_{ik}}{r_{ik}}\frac{\partial f_{ik}}{\partial r_{ik}} + \sum_{i\neq k}^N\bm{\nabla}_k d_{ik}(\mathbf{r})d_{ki}^{-1}(\mathbf{r}) \right)
\end{align}
where depending on particle $k$, we will either have the gradient of the Slater determinant to be for spin up or down.

\subsubsection{\texorpdfstring{$\alpha$}{a} and \texorpdfstring{$\beta$}{b} derivatives in the steepest descent algorithm}
From the lecture notes \husk{CITE COMPENDIUM}, we have that the derivative of a variational parameter is given by
\begin{align*}
	\frac{\partial E_L[c_m]}{\partial c_m} = 2 \left[ \left\langle E_L[c_m] \frac{\partial \ln \psi_L}{\partial c_m} \right\rangle - \langle E_L[c_m] \rangle \left\langle \frac{\partial \ln \psi_L}{\partial c_m} \right\rangle
	\right]
\end{align*}
where $c_m$ is the variational parameter we are studying. As for the two electron case, the brackets are indicating expectation values. The $\psi_L$ is given as,
\begin{align}
	\frac{\partial\ln \psi_L}{\partial c_m} = \frac{\partial\ln (\psi_{SD\uparrow})}{\partial c_m} + \frac{\partial\ln (\psi_{SD\downarrow})}{\partial c_m} + \frac{\partial\ln (\psi_{J})}{\partial c_m}
\end{align}
Since $\frac{\partial\ln\psi}{\partial c_m} = \frac{1}{\psi}\frac{\partial \psi}{\partial c_m}$, we have to find the partial derivatives of the wave functions with respect to the variational parameters. We begin by looking at the Jastrow factor.
\begin{align*}
	\frac{1}{\psi_J}\frac{\partial\psi_J}{\partial \beta} &= \frac{1}{\psi_J} \exp\left(\sum^N_{i<j}\frac{a_{ij}r_{ij}}{1+\beta r_{ij}}\right) \\
	&= \frac{1}{\psi_J}\left( - \sum_{i\neq j}^N \frac{a_{ij}r_ij^2}{(1+\beta r_{ij})^2} \right) \psi_J
\end{align*}
Which leads to,
\begin{align}
	\frac{1}{\psi_J}\frac{\partial\psi_J}{\partial \beta} = - \sum_{i\neq j}^N \frac{a_{ij}r_ij^2}{(1+\beta r_{ij})^2}
	\label{eq:jastrow-beta-derivative-n-particle}
\end{align}
Now, for the Slater determinant. Since the derivative is the same for either spin up or spin down, we will drop specifying the spin value. In finding this formula, the Jacobi's formula has been used.
\begin{align}
	\frac{\partial \psi_{SD}}{\partial\alpha} &= \sum^N_{i=1}\sum^N_{j=1} \frac{\partial\phi_j(\mathbf{r}_i)}{\partial \alpha} D^{-1}_{ji} \\
	\label{eq:slater-determinant-alpha-derivative}
\end{align}
Thus, we find the derivative of the single particle function as
\begin{align}
	\begin{split}
		\frac{\partial \phi_j(\mathbf{r}_i)}{\partial \alpha} &= \bigg[ \frac{1}{2}\sqrt{\frac{\omega}{\alpha}}\bigg( x_i H_{j_y} (\sqrt{\omega\alpha} y_i) \frac{\partial}{\partial \alpha}\left( H_{j_x} (\sqrt{\omega\alpha}x_i) \right) \\
		&- y_i H_{j_x} (\sqrt{\omega\alpha}x_i) \frac{\partial}{\partial \alpha}\left( H_{j_y} (\sqrt{\omega\alpha}y_i) \right)\bigg) \\
		&- \frac{\omega}{2}(x_i^2 + y_i^2) H_{j_x} (\sqrt{\omega\alpha}x_i) H_{j_y} (\sqrt{\omega\alpha} y_i) \bigg] \exp{\left(-\frac{\omega\alpha}{2}\left( x_i^2 + y_i^2 \right)\right)}
	\end{split}
	\label{eq:wf-sp-alpha-derivative}
\end{align}
The derivatives of the Hermite polynomials with respect to $\alpha$ are given by the Hermite derivative recursion relation seen in the appendix\eqref{eq:hermite-recursive-relation}, as
\begin{align}
	\frac{\partial}{\partial\alpha} \left(H_n(\sqrt{\omega\alpha}x)\right) = nx\sqrt{\frac{\omega}{\alpha}} H_{n-1}(\sqrt{\omega\alpha}x)
	\label{eq:hermite-alpha-derivative}
\end{align}

\subsection{Blocking}
When dealing with large Monte Carlo generated datasets, we will discover that they often are correlated. To remedy this, we introduce a statistical technique called \textit{blocking}. The idea behind this, can be summarized as following:
\begin{itemize}
	\item Define a set of different block sizes $\{N_b\}$, in which we will divide our data set into $n_b$ such blocks.
	\item Calculate the mean of each block with size $N_b$, such that the size of the data set is reduced the number of blocks $N_b$ which 
\end{itemize}
\husk{fiksefikse}


\subsection{Parallelization}
Using the MPIMPIMPIMPI?

\section{Implementation}


\subsection{TESTS}
\husk{2 electron case}

\subsection{Slater determinants}
An efficient way of updating the inverse of the Slater matrix can be found by \husk{SITER}. 

\begin{align}
D_{kj}^{-1}(\mathbf{x}^\text{new}) = \left\{
\begin{array}{ll}
	D_{kj}^{-1}(\mathbf{x}^{old}) - \frac{D_{ki}^{-1}(\mathbf{x}^{old})}{R}\sum^N_{l=1} D_{il}(\mathbf{x}^{new})D^{-1}_{lj}(\mathbf{x}^{old}) & \text{if } j\neq i \\
	\frac{D_{ki}^{-1}(\mathbf{x}^{old})}{R}\sum^N_{l=1} D_{il}(\mathbf{x}^{old})D^{-1}_{lj}(\mathbf{x}^{old}) & \text{if } j = i
\end{array}
\right.
\end{align}


\section{Results}



\section{Discussion and conclusion}



\section{Appendix}
\subsection{Hermite polynomials}
The first few Hermite polynomials in the physicists definition that will be used are given as,
\begin{align*}
	H_0(x) = 1
\end{align*}
\begin{align*}
	H_1(x) = 2x
\end{align*}
\begin{align*}
	H_2(x) = 4x^2 - 2
\end{align*}
\begin{align*}
	H_3(x) = 8x^3 - 12x
\end{align*}
\begin{align*}
	H_4(x) = 16x^4 - 48x^2 + 12
\end{align*}
Their derivatives is given as
\begin{align*}
	\frac{\partial H_0(x)}{\partial x} = 0
\end{align*}
\begin{align*}
	\frac{\partial H_1(x)}{\partial x} = 2
\end{align*}
\begin{align*}
	\frac{\partial H_2(x)}{\partial x} = 8x
\end{align*}
\begin{align*}
	\frac{\partial H_3(x)}{\partial x} = 24x^2 - 12
\end{align*}
\begin{align*}
	\frac{\partial H_4(x)}{\partial x} = 64x^3 - 96x
\end{align*}
Their second derivatives goes as
\begin{align*}
	\frac{\partial^2 H_0(x)}{\partial x^2} = 0
\end{align*}
\begin{align*}
	\frac{\partial^2 H_1(x)}{\partial x^2} = 0
\end{align*}
\begin{align*}
	\frac{\partial^2 H_2(x)}{\partial x^2} = 8
\end{align*}
\begin{align*}
	\frac{\partial^2 H_3(x)}{\partial x^2} = 48x
\end{align*}
\begin{align*}
	\frac{\partial^2 H_4(x)}{\partial x^2} = 192x^2 - 96
\end{align*}
For finding the $\alpha$ derivative, we can use the recursive derivative relation,
\begin{align}
	H'_n(x) = 2nH_{n-1}(x)
	\label{eq:hermite-recursive-relation}
\end{align}

\husk{Ha med polynomene som brukes}

\subsection{Integral formulas}
Solution to a Gaussian integral is given as,
\begin{align}
	\int \exp\left( - \omega x^2 \right) d x = \sqrt{\frac{\pi}{\omega}} = \sqrt{\pi}\omega^{-\frac{1}{2}}
	\label{eq:gaussian-integral}
\end{align}
Another symmetric Gaussian integral is given as,
\begin{align}
	\int x^2 \exp\left( - \omega x^2 \right) d x = \frac{\sqrt{\pi}}{2}\omega^{-\frac{3}{2}}
	\label{eq:gaussian-integral-x-squared}
\end{align}
\subsection{Jacobi's formula}
When finding the derivative of the Slater determinant, we will need the Jacobi formula,
\begin{align}
	\frac{d}{dt}\det A(t) = \mathrm{tr} \left( \mathrm{adj}(A(t))\frac{dA(t)}{dt} \right)
	\label{eq:jacobi-formula}
\end{align}

\husk{REFERANSER!!}

\end{document}